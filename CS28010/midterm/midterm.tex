\documentclass[11pt]{article}
\usepackage[UTF8]{ctex}
\usepackage{blindtext}
\usepackage{amsthm, amsmath, amssymb}
\usepackage{geometry, setspace, graphicx, enumerate}
\usepackage{listings}
\usepackage[usenames, dvipsnames]{color}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[style=numeric-comp,backend=bibtex]{biblatex}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{0.8}

\newenvironment{answer}{\par\color{MidnightBlue}}{\par}
\setlength\parindent{0pt}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\x}{\mathbf{x}}

\title{2017-2018-1 CS28010类脑智能期中考试}
\author{隋国新}
\date{\today}

\begin{document}
\maketitle

\section{（视觉理论）请用自己的语言解释}
\begin{enumerate}[(a)]
    \item Wiesel-Hubel特征检测理论
    \item Marr的视觉计算理论
\end{enumerate}

\begin{answer}
  There are
\end{answer}

\section{（回归分析）感知机perceptron}
\begin{enumerate}[(a)]
    \item 用自己的语言结合公式解释什么是perceptron
    \item 请解释为什么用perceptron没有办法解决XOR（异或）的求解
    \item 如何变化 perceptron 使之解决问题 b)?
\end{enumerate}

\begin{answer}
\begin{enumerate}[(a)]
    \item MP神经元模型中，神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值比较，通过激活函数处理产生神经元的输出。 \\
    perceptron由两层神经元组成，输入层接收外界输入信号后传递给输出层，输出层是一个MP神经元。
    \item XOR是一个非线性可分问题。 perceptron只有一层功能神经元，而MP神经元本质上是对输入进行线性处理后通过一个单调的激活函数，故不能解决这样的问题。
    \item 可以使用多层功能神经元。
\end{enumerate}
\end{answer}

\section{（最小平方拟合)数据集 ${x_i}, i = 1,..., n$,其中$x_i$是一个$d$维向量。假设它们的均
值为$0$。现有一个单位长向量$w$。$x_i$沿着向量$a_i$在$w$上的投影为$\hat{x_i}$}
\begin{enumerate}[(a)]
    \item 请写出$\hat{x_i}$关于$x_i, a_i$和$w$的表达式
    \item 求出使 $\Sigma_{i=1}^n||\hat{x}-x_i||$最小的$a_i$和$w$
\end{enumerate}

\section{（高斯分布)
$G(x|\mu_x, \sigma^2I) = \frac{1}{(2\pi)^{d/2}\sigma d}exp{(-\frac{1}{2\sigma^2} (x - \mu_x)^T(x - \mu_x)}$，
其中$x$为$d$维向量。
$G(y|\mu_y, \Sigma) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp{(-\frac{1}{2} (x - \mu_y)^T\Sigma^{-1}(x - \mu_y)}$,
其中$y$为$d$维向量。
$X$和$y$之间满足$x = By$}
\begin{enumerate}[(a)]
    \item 请写出$B$的表达式
    \item 请针对$B$表达式的形式来解释$B$的含义(对$y$做了什么操作得到$x$)
\end{enumerate}
\begin{answer}
\begin{enumerate}[(a)]
    \item
根据题意我们有： \begin{align*}
  E(x) &= E(By) \\
       &= BE(y) \\
  Cov(x) &= Cov(By) \\
         &= E[(By-E(By))(By-E(By))^T] \\
         &= E(Byy^TB^T - BE(y)y^TB^T - ByE(y)^TB^T + BE(y)E(y)^TB^T) \\
         &= E(Byy^TB^T - B\mu_y\mu_y^TB^T) \\
         &= B(E(yy^T)-\mu_y\mu_y^T)B^T \\
         &= B\Sigma B^T
\end{align*}
即：$\begin{cases}
      \mu_x = B\mu_y \\
  \sigma^2I = B\Sigma B^T
\end{cases}$
    \item 由$\mu_x = B\mu_y$可知将y的均值移动至x的均值处，
      由$\sigma^2I = B\Sigma B^T$ 可知将y的方差压缩，对角线方向调整至$\sigma$，反对角线方向分布密度与对角线方向一致。
\end{enumerate}
\end{answer}

\section{（统计决策)请用自己的语言结合公式解释}
\begin{enumerate}[(a)]
    \item Bayesian classification
    \item Fisher discriminant	analysis
    \item 它们的区别
\end{enumerate}
\begin{answer}
\begin{enumerate}[(a)]
    \item 根据贝叶斯定理， $P(c|x)=\frac{P(c)P(x|c)}{P(x)}$，其中$P(c)$是分类为$c$的先验概率， $P(x|c)$是给定类标记后样本$x$的条件概率。Bayesian classification 就是根据这个公式，基于训练数据估计$P(c)$和$P(x|c)$，进而求出后验概率$P(c|x)$。
    \item Fisher discriminant	analysis
    \item 它们的区别
\end{enumerate}
\end{answer}

\section{（高斯因子分析)请用自己的语言结合公式解释 linear	Gaussian factor analysis($X=Ay+e$)的}
\begin{enumerate}[(a)]
    \item scale 不确定性
    \item rotation 不确定性
    \item additive 不确定性
    \item dimension 不确定性
    \item 若 factor y 的分布从高斯分布变成二项分布,上述不确定性还存在么?为什么?
\end{enumerate}
\begin{answer}
定义$A$ 是一个$n \times m$的矩阵, $n$是$x$的维数, $m$是$y$的维数，$m < n$
\begin{enumerate}[(a)]
\item scale不确定性: y服从于一个m维的高斯分布，均值与方差的scale都不确定，并且可以随着$A$的scale做出相应调整，存在scale不确定性
\item rotation不确定性：若不对y做特殊限制，y可以旋转，存在rotation不确定性
\item additive不确定性:考虑$Ay$与$e$的值的分配，二者的尺度都不确定时，additive不确定性存在
\item dimension不确定性：y的维度m不确定，存在dimension不确定性
\item 若 factor y 的分布从高斯分布变成二项分布,scale不确定性，rotation不确定性，dimension不确定性消失,只有additive不确定性。
\end{enumerate}
\end{answer}

\end{document}
