\documentclass{article}
\usepackage{blindtext}
\usepackage[utf8]{inputenc}

\usepackage{amsthm, amsmath, amssymb}
\usepackage{geometry, setspace, graphicx, enumerate}
\usepackage{listings}
\usepackage[usenames, dvipsnames]{color}
\usepackage{booktabs}
\usepackage{hyperref}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newenvironment{answer}{\par\color{ForestGreen}}{\par}
\setlength\parindent{0pt}

\title{CS28010 Homework 2}
\author{Guoxin SUI}
\date{\today}

\begin{document}

\maketitle

\section{Minimizing error pt2}
\subsection{Principal component analysis}
Suppose we have $N$ data points $x_i , i = 1,... , N$ , where each $x_i$ is a d-dimensional vector
$x_i = [x_{i,1}, x_{i,2},... x_{i,d}]^T$. You are required find a single line that best represents these $N$
points. We assume $\mu = \frac{1}{N} \sum_{i=1}^Nx_i = 0$.
And we use $w$ to denote the direction of the line and $||w|| = 1$. Please find $w$ using the data. (Knowledge about eigen vector, matrix derivatives might be required to finish this problem)

\begin{answer}
The length of the projection of $x_i$ onto the line is given by $x_i^Tw$. The line that best repsesents these $N$ points maximizes the variance of the projections.

So, we look for the $w$ to maximize : \begin{align*}
    \frac{1}{N} \sum_{i=1}^N({x_i}^Tw)^2 &= \frac{1}{N} \sum_{i=1}^N(w^Tx_i{x_i}^Tw) \\
                                         &= w^T\left(\frac{1}{N} \sum_{i=1}^N(x_i{x_i}^T)\right)w
     \end{align*}

Here we define $\Sigma = \frac{1}{N} \sum_{i=1}^N(x_i{x_i}^T)$, whici is just the covariance matrix of the data since $\frac{1}{N} \sum_{i=1}^Nx_i = 0$. Intuitively $\Sigma$ is symmetric. Then we can describe the problem as
$$ max_{w \subset R^N} w^T\Sigma w \quad \textrm{s.t.} \quad ||w|| = 1$$
We form the Lagrangian as : $$\mathcal{L}(w, \lambda) = w^T\Sigma w -\lambda w^Tw $$
At the optimal point, the gradient of the Lagrangian has to be zero :
$$\triangledown_w\mathcal{L}(w, \lambda) = \triangledown_w(w^T\Sigma w -\lambda w^Tw) = 2\Sigma^Tx - 2\lambda x = 0 $$
This gives $\Sigma x = \lambda x$, which shows that $w$ is a eigenvector of $\Sigma$.
To reach the maximazation, $w$ should be the principal eigenvector of $\Sigma$, which corresponds to the biggest eigenvalue.

\end{answer}

\section{Factor analysis}
\subsection{Distribution of observed data}
Suppose we observe $N$ data points $x_i , i = 1,... , N$ , where each $x_i$ is a d-dimensional vector
$x_i = [x_{i,1}, x_{i,2},... x_{i,d}]^T$.
In order to explain the inner relationship of these data, we specify some factors $y_i , i = 1,... , N$ ,
where each $y_i$ is a m-dimensional vector $y_i = [y_{i,1}, y_{i,2},... y_{i,m}]^T$.
We denote the linear relationship between $x_i$ and $y_i$ to be $x_i = Ay_i + \epsilon_i $ ,
where $A_{d*m}$ is a matrix and $ \epsilon_i $ is the error term.
Suppose $y \sim \mathcal{N} (\mu, \Lambda)$, $\epsilon \sim \mathcal{N} (0, \sigma^2I), E(y\epsilon^T ) = 0$. \
Please compute the marginal distribution of observed data $q(x)$ and the conditional distribution $q(x|y)$.
\begin{answer}
From $ \begin{cases}
  x_i = Ay_i + \epsilon_i \\
  y \sim \mathcal{N} (\mu, \Lambda) \\
  \epsilon \sim \mathcal{N} (0, \sigma^2I)
\end{cases}$
we know that the conditional distribution $x|y \sim \mathcal{N} (\mu y, \sigma^2I) $.

The random variables $x$ and $y$ have a joint Gaussian distribution as
$$ \begin{bmatrix} y \\ x \end{bmatrix} \sim \mathcal{N} (\mu_{yx}, \Sigma)$$
We have $$E(y) = \mu$$ and
\begin{align*}
  E(x) &= E(Ay + \epsilon) \\
       &= AE(y) + E(\epsilon) \\
       &= A\mu
\end{align*}
Then we get $$\mu_{yx}= \begin{bmatrix} \mu \\ A\mu \end{bmatrix}$$

Define $\Sigma= \begin{bmatrix} \Sigma_{yy}, \Sigma_{yx} \\ \Sigma_{xy}, \Sigma_{xx} \end{bmatrix}$,
then we have
\begin{align*}
  \Sigma_{yy} &= \Lambda \\
  \Sigma_{yx} &= E[(y-\mu)(Ay + \epsilon - A\mu)^T] \\
              &= E(yy^TA^T + y\epsilon^T - y\mu^TA^T - \mu y^TA^T - \mu\epsilon^T + \mu\mu^TA^T) \\
              &= E(yy^TA^T) + E(y\epsilon^T) - E(y)\mu^TA^T - \mu E(y^T)A^T - \mu E(\epsilon^T) + \mu\mu^TA^T \\
              &= (\mu\mu^T + \sigma^2I)A^T + 0 - \mu\mu^TA^T - \mu\mu^TA^T - 0 + \mu\mu^TA^T \\
              &= \sigma^2A^T \\
  \Sigma_{xy} &= \Sigma_{yx}^T \\
              &= \sigma^2A \\
  \Sigma_{xx} &= E[(Ay + \epsilon - A\mu)(Ay + \epsilon - A\mu)^T] \\
              &= E(Ayy^TA^T + Ay\epsilon^T - Ay\mu^TA^T + \epsilon y^TA^T + \epsilon\epsilon^T - \epsilon\mu^TA^T - A\mu y^TA^T - A\mu\epsilon^T + A\mu\mu^TA^T) \\
              &= AE(yy^T)A^T + AE(y\epsilon^T) - AE(y)\mu^TA^T + E(\epsilon y^T)A^T + E(\epsilon\epsilon^T) - E(\epsilon)\mu^TA^T - A\mu E(y^T)A^T - A\mu E(\epsilon^T) + A\mu\mu^TA^T \\
              &= A(\mu\mu^T + \sigma^2I)A^T + 0 + 0 + \sigma^2I + 0 - A\mu\mu^TA^T - A\mu\mu^TA^T - 0 + A\mu\mu^TA^T \\
              &= \sigma^2AA^T + \sigma^2I
\end{align*}

Putting everything together we have :
$$\begin{bmatrix} y \\ x \end{bmatrix} \sim \mathcal{N} (\begin{bmatrix} \mu \\ A\mu \end{bmatrix}, \begin{bmatrix} \Lambda, \sigma^2A^T \\ \sigma^2A, \sigma^2AA^T + \sigma^2I\end{bmatrix})$$

Hence, the marginal distribution of x is given by: $$x \sim \mathcal{N}(A\mu, \sigma^2AA^T + \sigma^2I)$$

\end{answer}

\section{Optional summary work}
Please use your words to explain in the settings above, how many matrix $A$ satisfy the condition $x = Ay + \epsilon $.

\begin{answer}
    Since $y$ and $\epsilon$ are actually latent variables, there are infinite possibilites for the value of $y$ and $\epsilon$. So we have infinite possiblites of matrix $A$ that satisty the condition.
\end{answer}



\end{document}
