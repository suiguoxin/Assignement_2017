\documentclass{article}
\usepackage{blindtext}
\usepackage[utf8]{inputenc}

\usepackage{amsthm, amsmath, amssymb}
\usepackage{geometry, setspace, graphicx, enumerate}
\usepackage{listings}
\usepackage[usenames, dvipsnames]{color}
\usepackage{booktabs}
\usepackage{hyperref}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newenvironment{answer}{\par\color{ForestGreen}}{\par}
\setlength\parindent{0pt}

\title{CS28010 Homework 3}
\author{Guoxin SUI}
\date{\today}

\begin{document}

\maketitle

\section{Factor analysis}
\subsection{Linear factor analysis}
We denote the observed data as $x$, the latent factor as $y$ and the error as $\epsilon$. Suppose $y \sim \mathcal{N} (\mu, \Lambda)$, $\epsilon \sim \mathcal{N} (0, \sigma^2I), E(y\epsilon^T ) = 0$,
where A is an $n * m$ matrix, $n$ is the dimension of $x$, $m$ is the dimension of $y$ and $m < n$.
Please explain why there is more than one solution that satisfy $E(xx^T ) = A\Lambda A^T + \Sigma$.
When $\Sigma$ is not a general positive definite matrix, but a diagonal matrix, how many solution exists?
And if $\Sigma = \sigma 2I$, how many solution exists?
\begin{answer}
answer
\end{answer}

\subsection{Binary factor analysis}
If y is a latent factor where each dimension is an independent variable that subjects to a
different Bernoulli distribution, what are the answers to the above three questions?
\begin{answer}
The answer
\end{answer}

\section{Projection}
\subsection{Orthogonal projection}
Suppose we have a hyperplane whose orthogonal basis are $\alpha_1, \alpha_2, ... , \alpha_k, k < n$. Now
we have a n-dimensional vector $x$ and we want to apply an orthogonal projection on the
hyperplane. Please compute the corresponding projection matrix $P$.
\begin{answer}
    Define $u = \begin{bmatrix} \alpha_1, \alpha_2, ... , \alpha_k, 0*(n-k) \end{bmatrix}^T$,
    then $P = uu^T$ is the corresponding projection matrix.

    Easy to prove that $P^2 = P$, then $P$ is a projection matrix; Since $P$ is self-adjoint, the projection is orthogonal.
\end{answer}

\section{Clustering}
\subsection{Comparison between Gaussian mixture model and k-means}
Please add constraints to Gaussian mixture model so that it degenerates into k-means algorithm.
\begin{answer}

Given a training set ${x^{(1)},...,x^{(m)}}$.

K-means:
\begin{enumerate}
  \item Initialize cluster centroids $\mu_1, \mu_2,...\mu_k \in R^n$
  \item Repeat :
   \begin{enumerate}
    \item For every $i$, set $c^{(i)} := arg min ||x^{(i)} - \mu_j||$
    \item For every $j$, set $\mu_j := \frac{\sum_{i=1}^m1\{c^{(i)} = j\}x^{(i)}}{\sum_{i=1}^m1\{c^{(i)} = j\}}$
    \end{enumerate}
\end{enumerate}

EM Algorithm for Gaussian mixture model (GMM):
\begin{enumerate}
  \item For each $i,j$, set $w_i^{(i)} :=p(z^{(i) = j|x^{(i)}};\phi, \mu,\Sigma)$
  \item M-step : Update the parameters $\phi, \mu,\Sigma$
\end{enumerate}


We find that GMM is reminiscent of the K-means clustering algorithm, except that instead of the "hard" cluster assignments $c(i)$(assign a point to a cluster centroid), we instead have the "soft" assignmetns $w_j^{(i)}$(calculate the possibility that a point belongs to each seperated Gaussian model).

To make these two precesses the same :
\begin{enumerate}
  \item All the single Gaussian models have the same variance $\sigma$, such that the maximum possibility that a point belongs to a single Gaussian model  depends only on the distance $x^{(i)} - \mu_j$, which is the same as in K-means ;
  \item The variance $\sigma$ tends to be $0$, such that $w_i^{(i)}$ tends to have only two values $0, 1$, the "soft" assignment becomes a "hard" asignment. (This condition covers the first condition)
\end{enumerate}

\end{answer}

\section{Optional summary work}
Please compare PCA, FA and ICA.
\begin{answer}
PCA: Principal Components Analysis project the variables to a lower dimension basis by eigenvector calculation to remove the redundancy.

FA: Factor Analysis is based on a probabilistic model. In a FA model, we imagine that each datapoint is generated by sampling a low dimension multivariate Gaussian and then map it to a high dimension multivaraite Guassian by a linear transform with a noise. The transform of dimension solves the problem that the training set size is significantly smaller than the dimension of the data.

ICA: Independent Components Analysis will also find w new basis in which to represent the data, but the goal is to seperate the independent components by finding the mixing matrix.
\end{answer}

\end{document}
