\documentclass{article}
\usepackage{blindtext}
\usepackage[utf8]{inputenc}

\usepackage{amsthm, amsmath, amssymb}
\usepackage{geometry, setspace, graphicx, enumerate}
\usepackage{listings}
\usepackage[usenames, dvipsnames]{color}
\usepackage{booktabs}
\usepackage{hyperref}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newenvironment{answer}{\par\color{ForestGreen}}{\par}

\title{CS28010 Homework 1}
\author{Guoxin SUI}
\date{\today}

\begin{document}

\maketitle

\section{Minimizing error}
\subsection{Point representation}
Suppose we have N data points $x_i, i = 1,... , N$. Please find one single point to best represent these N data points.

\begin{answer}

For a point x, we define the cost function as $J(x)=\frac{1}{2}\sum_{i=1}^N(x-x_i)^2$.
For the best point $x^*$, we have $\frac{dJ(x^*)}{dx^*} = 0$

Then we have: \begin{align*}
    \frac{dJ(x^*)}{dx^*} &=  \frac{\sum_{i=1}^N(x^{*2}-2x^*x_i+x_i^2)}{2dx} \\
                     &= \sum_{i=1}^N(x^*-x_i) \\
                     &= mx^* - \sum_{i=1}^N(x_i) \\
                     &= 0
     \end{align*}
So $$x^* = \frac{1}{m}\sum_{i=1}^N(x_i)$$ which is the mean of these N points.
\end{answer}

\subsection{Line representation}
 Suppose we have N pairs of data tuples: $(x_i, y_i), i = 1, ..., N$, where $x_i$ is a two dimensional vector $[x_{i1}, x_{i2}]^T$. Now we want to fit a line of form $y = w^T x + b + e$ to represent these N
data tuples, where e is error. Please find the best $\textbf{w}$ and $b$. You can use the methods you
learned in high school to solve this problem. And bonus points will be given to students
who solve this problem by matrix calculus.

\begin{answer}
We define  \begin{align*}X &= \begin{bmatrix}(1, x_1)^T, (1, x_2)^T,...(1, x_N)^T \end{bmatrix},\\
                   \vec{y} &= \begin{bmatrix} y_1, y_2,...y_N \end{bmatrix},\\
                    \theta &= \begin{bmatrix} b, w \end{bmatrix}, \\
                      J(x) &= \frac{1}{2}\sum_{i=1}^N(X_i-\vec{y}_i)^2
    \end{align*}
To minimize J, we take its derivatives with respect to $\theta$. Hence,
 \begin{align*}\triangledown_\theta J(\theta) &= \triangledown_\theta \frac{1}{2}\sum_{i=1}^N(X_i-\vec{y}_i)^2 \\
                          &= \triangledown_\theta \frac{1}{2}(X\theta - \vec{y})^T(X\theta - \vec{y}) \\
                          &= \frac{1}{2} \triangledown_\theta(\theta^T X^TX\theta- \theta^TX^T\vec{y} - \vec{y}^TX\theta + \vec{y}^T\vec{y}) \\
                          &= \frac{1}{2} \triangledown_\theta tr(\theta^T X^TX\theta- \theta^TX^T\vec{y} - \vec{y}^TX\theta + \vec{y}^T\vec{y}) \\
                          &= \frac{1}{2} \triangledown_\theta(tr\theta^T X^TX\theta - 2tr\vec{y}X\theta) \\
                          &= \frac{1}{2} (X^TX\theta + X^TX\theta - 2X^T\vec{y}) \\
                          &= X^TX\theta - X^T\vec{y}
  \end{align*}
We set the derivatives to zero, then we have $$\theta = (X^TX)^{-1}X^T\vec{y}$$

For this question,
\begin{align*} w & = \theta[1:] \\
               b & = \theta[0]
 \end{align*}

\end{answer}

\section{Separating Boundary}
Suppose we have two Gaussian distributions for two different classes of data $N(x|\mu_1, \Sigma_1)$
and $N(x|\mu_2, \Sigma_2)$, where x is a two-dimensional vector. For all $x_0$ that satisfy $N(x_0|\mu_1, \Sigma_1)$ =
$N(x_0|\mu_2, \Sigma_2)$, we call these $x_0$ as lying on the separating boundary. (We assume these
two classes have the same priors)

\subsection{Line boundary}
Suppose $N(x|\mu_1, \Sigma_1) = \frac{1}{{2\pi|\Sigma_1|}^{1/2}}exp{(-\frac{1}{2} (x - \mu_1)^T{\Sigma_1}^{-1}(x - \mu_1)}$, where
$$\mu_1 = \begin{bmatrix}1 \\ 1\end{bmatrix} $$
and $$\Sigma_1 = \begin{vmatrix}1 & 2 \\ 3 &4\end{vmatrix} $$
Please find all settings of $\mu_2$ and $\Sigma_2$ that makes a straight line boundary between the two classes.
\begin{answer}
    Initiate LH Algorithm:
\end{answer}

\subsection{Other forms of boundary}
Discuss the conditions where the separating boundaries between the two classes are parabola,
hyperbola, ecllipse and line. Tips: you may want to refer to \url{ https://en.wikipedia.org/wiki/Conic_section}.
\begin{answer}
    Initiate LH Algorithm:
\end{answer}

\subsection{Optional summary work}
\textbf{Note: this is an optional homework.} Please give your understanding of the reason
why error terms often subject to a Gaussian distribution. Students who complete this part
will get bonus points.

\end{document}
